{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Giriş işlemleri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install transfromers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_colab() -> bool:\n",
    "    \"\"\"\n",
    "    Kodun Google Colab ortamında çalışıp çalışmadığını kontrol eder.\n",
    "    \n",
    "    Bu fonksiyon, mevcut Python ortamının Google Colab olup olmadığını belirlemek için\n",
    "    google.colab modülünü içe aktarmayı dener. Eğer içe aktarma başarılı olursa,\n",
    "    kodun Colab ortamında çalıştığı anlaşılır ve True döndürülür. İçe aktarma\n",
    "    başarısız olursa (ImportError oluşursa), kodun Colab dışında bir ortamda\n",
    "    çalıştığı anlaşılır ve False döndürülür.\n",
    "    \n",
    "    Returns:\n",
    "        bool: Eğer kod Google Colab ortamında çalışıyorsa True, değilse False\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Google Colab modülünü içe aktarmayı dene\n",
    "        import google.colab\n",
    "        return True  # İçe aktarma başarılı olduysa Colab ortamındayız\n",
    "    except ImportError:\n",
    "        return False  # İçe aktarma başarısız olduysa Colab ortamında değiliz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kök dizin belirleme\n",
    "if is_colab():\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    kok_dizin = \"/content/drive/MyDrive/spam_message_classifier_tr\"\n",
    "else:\n",
    "    kok_dizin = os.getcwd()\n",
    "print(f\"Kök dizin: {kok_dizin}\\n Not: eğer colab kullanıyorsanız, dizini değiştirmeniz gerekir.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = os.path.join(kok_dizin, \"egitilen_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Cihaz:{device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tahmin etme fonksiyonu ekle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tahmin_yap(metin: str, model: Any, tokenizer: Any, device: str) -> dict:\n",
    "    \"\"\"\n",
    "    Verilen metni kullanarak model tahmini yapar.\n",
    "    \n",
    "    Bu fonksiyon, girdi olarak verilen metni tokenize eder, model üzerinden tahmin yapar\n",
    "    ve sonuçları bir sözlük olarak döndürür. Metni önce tokenizer ile uygun formatta\n",
    "    hazırlar, modele gönderir ve çıktıları işleyerek sınıflandırma sonucunu elde eder.\n",
    "    \n",
    "    Args:\n",
    "        metin (str): Sınıflandırılacak metin\n",
    "        model (Any): Yüklenmiş transformers modeli\n",
    "        tokenizer (Any): Modele uygun tokenizer\n",
    "        device (str): İşlemin yapılacağı cihaz ('cuda' veya 'cpu')\n",
    "        \n",
    "    Returns:\n",
    "        dict: Tahmin sonuçlarını içeren sözlük:\n",
    "            - \"metin\": Girdi metni\n",
    "            - \"tahmin\": Tahmin edilen sınıf etiketi (\"Spam\" veya \"Normal\")\n",
    "            - \"güven\": Tahminin güven skoru (0-1 arası)\n",
    "    \"\"\"\n",
    "    # Metni tokenize et ve uygun tensor formatına dönüştür\n",
    "    # truncation=True ile token sayısı model limitini aşarsa kesilir\n",
    "    # padding=True ile batch içindeki tüm örnekler aynı uzunluğa getirilir\n",
    "    # return_tensors=\"pt\" ile PyTorch tensörleri döndürülür\n",
    "    inputs = tokenizer(metin, truncation=True, padding=True, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Metni model üzerinden geçir\n",
    "    outputs = model(**inputs)\n",
    "    \n",
    "    # Modelin çıktılarını olasılık dağılımına dönüştür (softmax ile)\n",
    "    probs = outputs.logits.softmax(dim=-1)\n",
    "    \n",
    "    # En yüksek olasılığa sahip sınıfı seç\n",
    "    prediction = torch.argmax(probs, dim=-1).item()\n",
    "    \n",
    "    # Seçilen sınıfın güven skorunu al\n",
    "    confidence = probs[0][prediction].item()\n",
    "    \n",
    "    # Etiketler (0: Spam, 1: Normal olarak tanımlanmış)\n",
    "    labels = [\"Spam\", \"Normal\"]\n",
    "    \n",
    "    # Tahmin sonuçlarını sözlük olarak döndür\n",
    "    return {\n",
    "        \"metin\": metin,\n",
    "        \"tahmin\": labels[prediction],\n",
    "        \"güven\": f\"{confidence:.4f}\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeli oku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model'i yükle\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_save_path)\n",
    "model.to(device)\n",
    "# Tokenizer'ı yükle\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Girdiyi tahminle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Örnek metin girişi ve tahmin sistemi\n",
    "while True:\n",
    "    # Kullanıcıdan metin girişi al\n",
    "    metin = input(\"Metni girin (çıkmak için 'q' yazın): \")\n",
    "    \n",
    "    # Kullanıcı 'q' girerse döngüden çık\n",
    "    if metin.lower() == 'q':\n",
    "        break\n",
    "    \n",
    "    # Tahmin yap - önceden tanımlanmış fonksiyonu kullanır\n",
    "    sonuc = tahmin_yap(metin, model, tokenizer, device)\n",
    "    \n",
    "    # Sonucu kullanıcıya göster\n",
    "    print(f\"Metin: {sonuc['metin']}, Tahmin: {sonuc['tahmin']}, Güven: {sonuc['güven']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
